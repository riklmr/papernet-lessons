{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neural net to find object center, reducing failure rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning to optimize a Neural Net, part 2\n",
    "This code goes with [this blogpost](https://medium.com/@mlrik/learning-to-optimize-a-neural-net-part-2-4397922d0243) (Draft)\n",
    "\n",
    "This experiment is designed to measure the effects of reinitializing the learned weights of a NN. The second post in blog explores different ways to reduce the failure rate.\n",
    "\n",
    "The failure rate was defined as \"proportion of models with loss>0.45 after 2000 epochs of training from scratch\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"PaperNet.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the initially designed NN has no convolutional layers. The Keras implementation looked like this:\n",
    "```python\n",
    "model.add(Dense(4, use_bias=True, activation=None, input_shape=(img_size,)))\n",
    "model.add(Dense(2, use_bias=True, activation='relu'))\n",
    "model.add(Dense(1, use_bias=False, activation=None))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous [blogpost](https://medium.com/@mlrik/learning-to-optimize-a-neural-net-d60e85135b49), I proposed to myself to reduce the failure rate with one or more of the following:\n",
    "1. activation functions (non-linearities) in all layers;\n",
    "2. more layers;\n",
    "3. more units per layer;\n",
    "4. a bigger data set (add a bit of noise to the pixel values);\n",
    "5. bigger images;\n",
    "6. select the best models early in the process, clone them to maintain their numbers, add some randomness to the weights for variety, cull the others, train some more, repeat;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "import gc\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a wee warning\n",
    "This notebook keeps everything in memory. You might run into problems when increasing img_size or sprawl beyond a certain limit.\n",
    "### a big warning\n",
    "My code below is sloppy. Some functions rely on global variables to work. This is prototyping, no more, no less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create image functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_blob_images(img_size = 3, normx = False, normy = False):\n",
    "    \"\"\"create images (X) and labels (Y) indicating center of the object\n",
    "    each image has an object and some empty space\n",
    "    images have height of 1 pixel and width of img_size pixels\"\"\"\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    if normx:\n",
    "        pixelvalue = 1\n",
    "    else:\n",
    "        pixelvalue = 2*img_size\n",
    "    #\n",
    "    for blob_size in range(1, img_size):\n",
    "        for start in range(0, img_size - blob_size + 1):\n",
    "            img = [0] * img_size\n",
    "            img[start:start+blob_size] = [pixelvalue] * blob_size\n",
    "            center = start + blob_size/2\n",
    "            if normy:\n",
    "                center = center / img_size\n",
    "            X.append(img)\n",
    "            Y.append(center)\n",
    "        #\n",
    "    #\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    Y = Y.reshape(Y.shape[0], 1)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(x,y_true, normy=False):\n",
    "    \"\"\"visualize an image (x) with the center of object (y_true) using matplotlib\"\"\"\n",
    "    if normy:\n",
    "        y_true = y_true * img_size\n",
    "    img = x.reshape(1,img_size)\n",
    "    plt.imshow(img, cmap='binary', extent=[0,img_size,0,1])\n",
    "    plt.plot([y_true,y_true],[0,1], color='r', linewidth=2)\n",
    "    plt.xticks(np.arange(0,img_size+1))\n",
    "    plt.yticks([])\n",
    "    plt.grid(axis='x')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create X and Y: images and blob center values\n",
    "For a given size of the image. Display one such image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 3\n",
    "X,Y = make_blob_images(img_size, normx=True, normy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute this cell again and again to see more samples of X,Y\n",
    "x = X[i]\n",
    "y=Y[i]\n",
    "\n",
    "plot_img(x,y, normy=True)\n",
    "i = (i + 1) % len(X) # =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(Y))\n",
    "print(np.std(Y))\n",
    "print(np.std(Y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot all images in one plot\n",
    "w = len(X[0])\n",
    "h = len(X)\n",
    "nominator = w * 2\n",
    "nom = \"{}th\".format(nominator)\n",
    "enums = []\n",
    "for i in (Y*nominator).astype(int):\n",
    "    enums.append(\"{}/{}\".format(i[0], nom))\n",
    "#\n",
    "\n",
    "# plt.figure(figsize=(w/2,h/2))\n",
    "plt.imshow(X, cmap='binary', extent=[0,w,0,h])\n",
    "plt.grid(axis='both')\n",
    "\n",
    "plt.xticks(np.arange(0,w),)\n",
    "plt.yticks(np.arange(0,h+1), enums) # TODO hardcoded values into vars\n",
    "plt.xlabel(\"pixel\")\n",
    "plt.ylabel(\"horizontal center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### side note on the perfect outputs and the perfect loss\n",
    "An object in 3-pixel-images has a horizontal center at exactly 1/6th of the width of the image. Or at a multiple of 1/6th. There are no images in this set with object width of zero or six.\n",
    "\n",
    "The mean of all possible blob centers lies at exactly 0.500 (3/6th). The distance between two centers is always 0.1667 (1/6th). \n",
    "\n",
    "Any model that can predict all 5 centers within half that distance (*) of the true center, is performing \"good enough\". This is to say: a Mean Absolute Error below 0.0833 (1/12th) is a reasonable measure of a good model. However, a MAE<0.0833 does not promise if the model will predict correctly for all images. It is only an average error.\n",
    "\n",
    "To call a model perfect, we would need to assess every single prediction for every possible image. And determine if they all stay below the calculated threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_enough_loss = 1 / (img_size * 4)\n",
    "print(\"good enough loss {:.4f}\".format(good_enough_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(*) Let's just ignore the edge cases where a blob lies at the edge of the image, where the model predicts its center beyond the width of the image and where we round that off to the nearest realistic value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create functions for model handling\n",
    "Activate the make_model() function that you want to experiment with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unchanged PaperNet version\n",
    "def make_model(img_size=3, nn_depth=3, ll_width=3):\n",
    "    \"\"\"\n",
    "    create a NN model according to the PaperNet architecture\n",
    "    input_shape depends on img_size\n",
    "    output_shape is 1\n",
    "    \"\"\"\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(4, use_bias=True, activation=None , input_shape=(img_size,) ) )\n",
    "    model.add(keras.layers.Dense(2, use_bias=True, activation='relu' ) )\n",
    "    model.add(keras.layers.Dense(1, use_bias=False, activation=None ) )\n",
    "    model = compile_model(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# version actnone\n",
    "def make_model(img_size=3, nn_depth=3, ll_width=3):\n",
    "    \"\"\"\n",
    "    create a NN model kinda like the PaperNet architecture\n",
    "    varies in activation functions and/or bias definitions\n",
    "    hack this function as experiment demands\n",
    "    input_shape depends on img_size\n",
    "    output_shape is 1\n",
    "    \"\"\"\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(4, use_bias=True, activation=None , input_shape=(img_size,) ) )\n",
    "    model.add(keras.layers.Dense(2, use_bias=True, activation=None ) )\n",
    "    model.add(keras.layers.Dense(1, use_bias=True, activation=None ) )\n",
    "    model = compile_model(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# version all_relu\n",
    "def make_model(img_size=3, nn_depth=3, ll_width=3):\n",
    "    \"\"\"\n",
    "    create a NN model kinda like the PaperNet architecture\n",
    "    varies in activation functions and/or bias definitions\n",
    "    hack this function as experiment demands\n",
    "    input_shape depends on img_size\n",
    "    output_shape is 1\n",
    "    \"\"\"\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(4, use_bias=True, activation='relu' , input_shape=(img_size,) ) )\n",
    "    model.add(keras.layers.Dense(2, use_bias=True, activation='relu' ) )\n",
    "    model.add(keras.layers.Dense(1, use_bias=True, activation='relu' ) )\n",
    "    model = compile_model(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# version all_sigmoid\n",
    "def make_model(img_size=3, nn_depth=3, ll_width=3):\n",
    "    \"\"\"\n",
    "    create a NN model kinda like the PaperNet architecture\n",
    "    varies in activation functions and/or bias definitions\n",
    "    hack this function as experiment demands\n",
    "    input_shape depends on img_size\n",
    "    output_shape is 1\n",
    "    \"\"\"\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(4, use_bias=True, activation='sigmoid' , input_shape=(img_size,) ) )\n",
    "    model.add(keras.layers.Dense(2, use_bias=True, activation='sigmoid' ) )\n",
    "    model.add(keras.layers.Dense(1, use_bias=True, activation='sigmoid' ) )\n",
    "    model = compile_model(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# version hodgepodge\n",
    "def make_model(img_size=3, nn_depth=3, ll_width=3):\n",
    "    \"\"\"\n",
    "    create a NN model kinda like the PaperNet architecture\n",
    "    varies in activation functions and/or bias definitions\n",
    "    hack this function as experiment demands\n",
    "    input_shape depends on img_size\n",
    "    output_shape is 1\n",
    "    \"\"\"\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(4, use_bias=True, activation='linear' , input_shape=(img_size,) ) )\n",
    "    model.add(keras.layers.Dense(2, use_bias=True, activation='relu' ) )\n",
    "    model.add(keras.layers.Dense(1, use_bias=True, activation='sigmoid' ) )\n",
    "    model = compile_model(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# version base_n\n",
    "def make_model(img_size=3, nn_depth=3, ll_width=3):\n",
    "    \"\"\"\n",
    "    create a NN model inspired by the PaperNet architecture\n",
    "    depth of NN varies with nn_depth\n",
    "    number of units is 4 per layer, except in the final two layers\n",
    "    input_shape depends on img_size\n",
    "    output_shape is 1\n",
    "    \"\"\"\n",
    "    nn_depth = max(3, nn_depth)\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # the original PaperNet had 4,2,1 units-per-layer:\n",
    "    # to keep with that (for now) we decide to give any additional layers exactly 4 units\n",
    "    for l in range(nn_depth-2):\n",
    "        model.add(keras.layers.Dense(4, use_bias=True, activation=None, input_shape=(img_size,) ) )\n",
    "    #\n",
    "    model.add(keras.layers.Dense(2, use_bias=True, activation=None ) )\n",
    "    model.add(keras.layers.Dense(1, use_bias=True, activation=None ) )\n",
    "    model = compile_model(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# version \"all layers are equally sized\"\n",
    "def make_model(img_size=3, nn_depth=3, ll_width=3):\n",
    "    \"\"\"\n",
    "    create a NN model inspired by the PaperNet architecture\n",
    "    depth of NN varies with nn_depth\n",
    "    number of units is 4 per layer, except in the final/output layer\n",
    "    input_shape depends on img_size\n",
    "    output_shape is 1\n",
    "    \"\"\"\n",
    "    nn_depth = max(3, nn_depth)\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    for l in range(nn_depth-1):\n",
    "        model.add(keras.layers.Dense(4, use_bias=True, activation=None, input_shape=(img_size,) ) )\n",
    "    #\n",
    "    model.add(keras.layers.Dense(1, use_bias=True, activation=None ) )\n",
    "    model = compile_model(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# version base_n_x_l\n",
    "# variable depth AND variable width\n",
    "def make_model(img_size=3, nn_depth=3, ll_width=3):\n",
    "    \"\"\"\n",
    "    create a NN model inspired by the PaperNet architecture\n",
    "    depth of NN varies with nn_depth\n",
    "    number of units varies with ll_width, except in the final/output layer\n",
    "    input_shape depends on img_size\n",
    "    output_shape is 1\n",
    "    \"\"\"\n",
    "    nn_depth = max(3, nn_depth)\n",
    "    ll_width = max(3, ll_width)\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    for l in range(nn_depth-1):\n",
    "        model.add(keras.layers.Dense(ll_width, use_bias=True, activation=None, input_shape=(img_size,) ) )\n",
    "    #\n",
    "    model.add(keras.layers.Dense(1, use_bias=True, activation=None ) )\n",
    "    model = compile_model(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_model(parent):\n",
    "    \"\"\"returns exact, compiled, copy of model with weights and all\"\"\"\n",
    "    model = keras.models.clone_model(parent)\n",
    "    W = parent.get_weights()\n",
    "    model.set_weights(W)\n",
    "    model = compile_model(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    \"\"\"\n",
    "    returns compiled PaperNet model with SGD optimizer and hard coded hyperparameters\n",
    "    loss is defined as MAE (mean absolute error)\n",
    "    \"\"\"\n",
    "    model.compile(optimizer=keras.optimizers.SGD(lr=.01), loss=keras.losses.mean_absolute_error, metrics=['mean_absolute_percentage_error'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs = 100):\n",
    "    \"\"\"\n",
    "    trains model using batch_size=len(X)\n",
    "    returns training history object\n",
    "    \"\"\"\n",
    "    history = model.fit(x=X, y=Y, epochs=epochs, batch_size=len(X), verbose=False)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    \"\"\"evaluates model, returns loss and metrics\"\"\"\n",
    "    return model.evaluate(x=X, y=Y, verbose=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# static version, model size does not change\n",
    "def waxon_waxoff(parent=None, sprawl=1, epochs=100): \n",
    "    \"\"\"\n",
    "    creates sprawl number of models, each with different random weights\n",
    "    if parent model is given, will clone_model(parent) instead, resulting in sprawl identical models\n",
    "    each model is trained a given number of epochs\n",
    "    returns three lists with corresponding index: \n",
    "    children (contains model objects),\n",
    "    stats (contains loss values),\n",
    "    curves (contains history objects)\n",
    "    \"\"\"\n",
    "    children = []\n",
    "    stats = []\n",
    "    curves = []\n",
    "    \n",
    "    print(\"train {} models, train each model for {} epochs\".format(sprawl, epochs))\n",
    "    for r in range(sprawl):\n",
    "        print(\" model:\", r)\n",
    "        if parent == None:\n",
    "            model = make_model(img_size=img_size)\n",
    "        else:\n",
    "            model = clone_model(parent)\n",
    "        #\n",
    "        history = train(model=model, epochs = epochs)\n",
    "        loss = evaluate(model=model)[0]\n",
    "\n",
    "        children.append(model)\n",
    "        stats.append(loss)\n",
    "        curves.append(history)\n",
    "    #\n",
    "    return children, stats, curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic version, model size changes as requested\n",
    "def waxon_waxoff(parent=None, sprawl=1, epochs=100, nn_depth=3, ll_width=3): \n",
    "    \"\"\"\n",
    "    without parent, creates sprawl number of models, each with different random weights\n",
    "    if parent model is given, will clone_model(parent) instead, resulting in sprawl identical models\n",
    "    each model is trained a given number of epochs\n",
    "    returns three lists with corresponding index: \n",
    "    children (contains model objects),\n",
    "    stats (contains loss values),\n",
    "    curves (contains history objects)\n",
    "    \"\"\"\n",
    "    children = []\n",
    "    stats = []\n",
    "    curves = []\n",
    "    \n",
    "    print(\"train {} models, train each model for {} epochs\".format(sprawl, epochs))\n",
    "    for r in range(sprawl):\n",
    "        print(\" model:\", r)\n",
    "        if parent == None:\n",
    "            model = make_model(img_size=img_size, nn_depth=nn_depth, ll_width=ll_width)\n",
    "        else:\n",
    "            model = clone_model(parent)\n",
    "        #\n",
    "        history = train(model=model, epochs = epochs)\n",
    "        loss = evaluate(model=model)[0]\n",
    "\n",
    "        children.append(model)\n",
    "        stats.append(loss)\n",
    "        curves.append(history)\n",
    "    #\n",
    "    return children, stats, curves\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create functions for reviewing experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def report_experiment(version='baseline'):\n",
    "    \"\"\"\n",
    "    report on an experiment with all the trimmings\n",
    "    \"\"\"\n",
    "    print(\"Report for experiment titled \\'{}\\'\".format(version))\n",
    "    print(\"mean loss: {:.4f}  std dev loss: {:.4f}\".format(np.mean(stats[version]), np.std(stats[version])) )\n",
    "    print(\"best loss: {:.4f}    worst loss: {:.4f}\".format(min(stats[version]), max(stats[version])) )\n",
    "    print(\"best model: {}      worst model: {}\".format(best[version], worst[version]))\n",
    "    a=np.array(stats[version])\n",
    "    b = a > 0.45\n",
    "    print(\"failure rate for version {}: {}\".format(version, b.sum() /len(b)))\n",
    "    \n",
    "    # plot predictions by Best model\n",
    "    model = generation[version][best[version]]\n",
    "    y_hat = model.predict(x=X).flatten()\n",
    "    pos = range(len(Y.flatten()))\n",
    "    plt.bar(pos, Y.flatten(), -.8, align='center', color='black', label='true')\n",
    "    plt.bar(pos, y_hat, .3, align='edge', color='#3a9362', label='predict')\n",
    "    plt.title(\"{} epochs per training run\\npredictions by best of {} models\".format(epochs[version], sprawl[version]))\n",
    "    plt.xlabel(\"sample image\")\n",
    "    plt.ylabel(\"output\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # plot predictions by Worst model\n",
    "    model = generation[version][worst[version]]\n",
    "    y_hat = model.predict(x=X).flatten()\n",
    "    pos = range(len(Y.flatten()))\n",
    "    plt.bar(pos, Y.flatten(), -.8, align='center', color='black', label='true')\n",
    "    plt.bar(pos, y_hat, .3, align='edge', color='#3a9362', label='predict')\n",
    "    plt.title(\"{} epochs per training run\\npredictions by worst of {} models\".format(epochs[version], sprawl[version]))\n",
    "    plt.xlabel(\"sample image\")\n",
    "    plt.ylabel(\"output\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # plot all losses for all models in the experiment\n",
    "    pos = range(len(stats[version]))\n",
    "    plt.bar(pos,stats[version])\n",
    "    plt.title(\"losses in order, trained {} models for {} epochs\".format(sprawl[version], epochs[version]))\n",
    "    plt.xlabel(\"model\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.show()\n",
    "\n",
    "    # plot histogram of losses of all models in the generation\n",
    "    plt.hist(x=stats[version], bins=9)\n",
    "    plt.title(\"losses histogram, trained {} models for {} epochs\".format(sprawl[version], epochs[version]))\n",
    "    plt.xlabel(\"loss\")\n",
    "    plt.ylabel(\"absolute freq.\")\n",
    "    plt.show()\n",
    "\n",
    "    # plot learning curves for all models in one graph\n",
    "    nicknames = range(len(curves[version]))\n",
    "    plt.figure(figsize=(14,6))\n",
    "    if len(nicknames) > 20:\n",
    "        gray = True\n",
    "    else:\n",
    "        gray = False\n",
    "    #\n",
    "    for c in nicknames:\n",
    "        h = curves[version][c].history['loss']\n",
    "        if gray:\n",
    "            plt.plot(h, label=c, color='gray', alpha=0.1)\n",
    "        else:\n",
    "            plt.plot(h, label=c)\n",
    "        #\n",
    "        plt.xscale(\"log\")\n",
    "    #\n",
    "    plt.title(\"{} models trained for {} epochs\\nexperiment titled {}\".format(sprawl[version], epochs[version], version))\n",
    "    if not gray:\n",
    "        plt.legend()\n",
    "    #\n",
    "    plt.xlabel(\"epoch (log scale)\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_experiment(version='baseline'):\n",
    "    \"\"\"\n",
    "    summary of an experiment, just the numbers\n",
    "    returns failure_rate, best_loss\n",
    "    \"\"\"\n",
    "    a=np.array(stats[version])\n",
    "    b = a > 0.45\n",
    "    failure_rate = b.sum() /len(b)\n",
    "    best_loss = min(stats[version])\n",
    "\n",
    "    return failure_rate, best_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_model(model):\n",
    "    \"\"\"\n",
    "    summary of a model, just the numbers\n",
    "    returns accuracy, error_max\n",
    "    \"\"\"\n",
    "    # accuracy of model\n",
    "    # is calculated as the proportion of images in X that the model predicts within good_enough margin\n",
    "    # predict for the entire set X\n",
    "    y_hat = model.predict(x=X)\n",
    "    # make sure we work with flatten()ed arrays\n",
    "    y = Y.flatten()\n",
    "    y_hat = y_hat.flatten()\n",
    "    # subtract\n",
    "    errors = y - y_hat\n",
    "    # get rid of minus signs\n",
    "    absolute_errors = np.abs(errors)\n",
    "    # create a binary map of the array absolute_errors (better vs worse than the norm)\n",
    "    b = absolute_errors < good_enough_loss\n",
    "    # count predictions that are good_enough, divide by total number of predictions\n",
    "    accuracy = b.sum() / len(b)\n",
    "    \n",
    "    # what is the biggest error in the prediction\n",
    "    error_max = np.max(absolute_errors)\n",
    "\n",
    "    return accuracy, error_max\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_model(model):\n",
    "    \"\"\"\n",
    "    report on a model with graphs and stats\n",
    "    \"\"\"\n",
    "    \n",
    "    # check model's architecture\n",
    "    model.summary()\n",
    "    \n",
    "    # make a full prediction on the entire training set\n",
    "    y_hat = model.predict(x=X).flatten()\n",
    "    y = Y.flatten()\n",
    "\n",
    "    # compute errors on all individual images and the mean absolute error\n",
    "    e = y - y_hat\n",
    "    ae = np.abs(e)\n",
    "    mae=np.mean(ae)\n",
    "    \n",
    "    # prepare for the graph\n",
    "    bot_margin = y - good_enough_loss\n",
    "    margin = [good_enough_loss*2] * len(y)\n",
    "    pos = range(len(y))\n",
    "\n",
    "    # plot bar graph of ground truth, acceptable margins, predictions, errors\n",
    "    plt.bar(pos, margin, .5, bot_margin, align='center', color='gray', alpha=.7)\n",
    "    plt.bar(pos, y, -.8, align='center', color='black', label='true', alpha=.7)\n",
    "    plt.bar(pos, y_hat, .3, align='edge', color='#3a9362', label='predict')\n",
    "    plt.bar(pos, e, .1, y_hat, align='edge', color='orange', label='error')\n",
    "\n",
    "    plt.title(\"version {} MAE={:.4f}\".format(version, mae))\n",
    "    plt.xlabel(\"sample image\")\n",
    "    plt.ylabel(\"output\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## improve PaperNet failure rate: different approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from here on, we store results of experiments in these dicts:\n",
    "generation = {}\n",
    "stats = {}\n",
    "curves = {}\n",
    "best = {}\n",
    "worst = {}\n",
    "# experiment's parameters are also stored in dicts:\n",
    "sprawl = {}\n",
    "epochs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the baseline from the first notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## name the experiment\n",
    "version = 'papernet' #  a label to give the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run the experiment, make sure to activate the correct cell with make_model()\n",
    "sprawl[version] = 12\n",
    "epochs[version] = 1400\n",
    "generation[version], stats[version], curves[version] = waxon_waxoff(sprawl = sprawl[version], epochs=epochs[version] )\n",
    "best[version]  = np.argmin(stats[version])\n",
    "worst[version] = np.argmax(stats[version])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "report_experiment(version=version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_model(generation[version][best[version]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This would be where you hack make_model() to do something different (or something similar, it's  up to you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset dicts:\n",
    "generation = {}\n",
    "stats = {}\n",
    "curves = {}\n",
    "best = {}\n",
    "worst = {}\n",
    "sprawl = {}\n",
    "epochs = {}\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "version = 'all_relu' # don't forget to change make_model() first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## run the experiment\n",
    "sprawl[version] = 13\n",
    "epochs[version] = 3200\n",
    "generation[version], stats[version], curves[version] = waxon_waxoff(sprawl = sprawl[version], epochs=epochs[version] )\n",
    "best[version]  = np.argmin(stats[version])\n",
    "worst[version] = np.argmax(stats[version])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "report_experiment(version='all_relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions by hand picked model\n",
    "i=11\n",
    "# i += 1 #=0\n",
    "\n",
    "model = generation[version][i]\n",
    "report_model(model)\n",
    "summary_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the best model and clone, use the clone to parent more models\n",
    "# m = clone_model(generation[version][best[version]])\n",
    "m = clone_model(generation[version][11] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run the experiment\n",
    "sprawl[version] = 1\n",
    "epochs[version] = 6000\n",
    "generation[version], stats[version], curves[version] = waxon_waxoff(parent=m, sprawl = sprawl[version], epochs=epochs[version] )\n",
    "best[version]  = np.argmin(stats[version])\n",
    "worst[version] = np.argmax(stats[version])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "report_experiment(version='all_relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = generation[version][0]\n",
    "report_model(model)\n",
    "summary_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add more layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try one extra layer, 4 layers in total\n",
    "# img -> FC4 -> FC4 -> FC2 -> FC1 = output\n",
    "version = 'base4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time\n",
    "## run the experiment, activate the corresponding make_model()\n",
    "sprawl[version] = 40\n",
    "epochs[version] = 800\n",
    "generation[version], stats[version], curves[version] = waxon_waxoff(sprawl = sprawl[version], \n",
    "                                                                    epochs=epochs[version],\n",
    "                                                                    nn_depth=4,\n",
    "                                                                    ll_width=4,\n",
    "                                                                   )\n",
    "best[version]  = np.argmin(stats[version])\n",
    "worst[version] = np.argmax(stats[version])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "report_experiment(version='base4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the best model and clone, use the clone to parent more models\n",
    "m = clone_model(generation[version][best[version]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run the experiment\n",
    "sprawl[version] = 1\n",
    "epochs[version] = 100000\n",
    "generation[version], stats[version], curves[version] = waxon_waxoff(parent=m, sprawl = sprawl[version], epochs=epochs[version] )\n",
    "best[version]  = np.argmin(stats[version])\n",
    "worst[version] = np.argmax(stats[version])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "report_experiment(version='base4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_experiment(version='base4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train models of varying depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More units per layer, starting by equalizing the number across layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# change make_model() again please\n",
    "for nn_depth in [3,4,5,6,7,8,9]:\n",
    "    version = \"base_{}x4\".format(nn_depth)\n",
    "    \n",
    "    ## run the experiment\n",
    "    sprawl[version] = 11\n",
    "    epochs[version] = 2200\n",
    "    generation[version], stats[version], curves[version] = waxon_waxoff(sprawl = sprawl[version], \n",
    "                                                                        epochs=epochs[version],\n",
    "                                                                        nn_depth=nn_depth,\n",
    "                                                                        ll_width=4,\n",
    "                                                                       )\n",
    "    best[version]  = np.argmin(stats[version])\n",
    "    worst[version] = np.argmax(stats[version])\n",
    "    failure_rate, best_loss = summary_experiment(version=version)\n",
    "    print(\"  {} layers in experiment {}, failure rate={:.4f}, best loss={:.4f}\".format(nn_depth, version, failure_rate, best_loss))\n",
    "    accuracy, error_max = summary_model(generation[version][best[version]])\n",
    "    print(\"  best model ({}) has accuracy {:.4f} and a max error of {:.4f}\".format(best[version], accuracy, error_max))\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generation.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "topscores = []\n",
    "worstpreds = []\n",
    "losses = []\n",
    "labels = ['base_3x4', 'base_4x4', 'base_5x4', 'base_6x4'] #, 'base_7x4', 'base_8x4', 'base_9x4']\n",
    "for version in labels:\n",
    "    loss = np.min(stats[version])\n",
    "    accuracy, error_max = summary_model(generation[version][best[version]] )\n",
    "    losses.append(loss)\n",
    "    topscores.append(accuracy)\n",
    "    worstpreds.append(error_max)\n",
    "#\n",
    "C = [1] * len(labels)\n",
    "G = [good_enough_loss] * len(labels)\n",
    "pos = range(len(labels))\n",
    "plt.bar(pos, C, color='darkred')\n",
    "plt.bar(pos, topscores, color='#3a9362')\n",
    "plt.xticks(pos, labels)\n",
    "plt.title('accuracy of best model in experiment')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(pos, G, width=1, color='gray', alpha=.8, label='good enough')\n",
    "plt.bar(pos, loss, width=1, color='blue', alpha=.4, label='model loss')\n",
    "plt.bar(pos, worstpreds, width=.3, color='orange', alpha=.8, align='edge', label='loss on worst prediction')\n",
    "plt.xticks(pos, labels)\n",
    "plt.legend()\n",
    "plt.title('worst prediction compared to model\\'s overall loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "report_experiment(version='base_3x4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "report_model(generation['base_7x4'][best['base_7x4']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Fat Experiment\n",
    "Run 49 experiments to explore a 7x7 matrix of combo's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset dicts:\n",
    "generation = {}\n",
    "stats = {}\n",
    "curves = {}\n",
    "best = {}\n",
    "worst = {}\n",
    "sprawl = {}\n",
    "epochs = {}\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# change make_model() and waxon_waxoff() as necessary\n",
    "big = [3,4,5,6,7,8,9]\n",
    "small = [3,4]\n",
    "beyondbig = [9,10,11,12,13]\n",
    "\n",
    "depths = big\n",
    "widths = big\n",
    "\n",
    "for nn_depth in depths:\n",
    "    for ll_width in widths:\n",
    "        version = \"base_{}l_{}u\".format(nn_depth, ll_width)\n",
    "        print(\"experiment\", version)\n",
    "\n",
    "        ## run the experiment\n",
    "        sprawl[version] = 8\n",
    "        epochs[version] = 1500\n",
    "        generation[version], stats[version], curves[version] = waxon_waxoff(sprawl = sprawl[version], \n",
    "                                                                            epochs=epochs[version],\n",
    "                                                                            nn_depth=nn_depth,\n",
    "                                                                            ll_width=ll_width,\n",
    "                                                                           )\n",
    "        best[version]  = np.argmin(stats[version])\n",
    "        worst[version] = np.argmax(stats[version])\n",
    "    #\n",
    "#\n",
    "#  4 experiments in about 12 minutes\n",
    "# 49 experiments in about 100 minutes\n",
    "# 35 experiments in about 70 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = len(depths)\n",
    "w = len(widths)\n",
    "xticks = range(min(widths), max(widths)+1)\n",
    "yticks = range(min(depths), max(depths)+1)\n",
    "y_offset = min(yticks)\n",
    "x_offset = min(xticks)\n",
    "\n",
    "em_map = np.zeros((d,w))\n",
    "ac_map = np.zeros((d,w))\n",
    "ls_map = np.zeros((d,w))\n",
    "\n",
    "for nn_depth in depths:\n",
    "    for ll_width in widths:\n",
    "        version = \"base_{}l_{}u\".format(nn_depth, ll_width)\n",
    "\n",
    "        if version in generation.keys():\n",
    "            model = generation[version][best[version]]\n",
    "            accuracy, error_max = summary_model(model)\n",
    "            failure_rate, best_loss = summary_experiment(version=version)\n",
    "            em_map[nn_depth-y_offset,ll_width-x_offset] = error_max\n",
    "            ac_map[nn_depth-y_offset,ll_width-x_offset] = accuracy\n",
    "            ls_map[nn_depth-y_offset,ll_width-x_offset] = best_loss           \n",
    "        #\n",
    "    #\n",
    "#\n",
    "plt.figure(figsize=(9,9))\n",
    "plt.imshow(em_map,cmap='inferno')\n",
    "plt.yticks(range(d),yticks)\n",
    "plt.ylabel('depth')\n",
    "plt.xlabel('width')\n",
    "plt.xticks(range(w),xticks)\n",
    "plt.colorbar(shrink=.8)\n",
    "plt.title('error on hardest prediction by best model')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "plt.imshow(ls_map,cmap='ocean')\n",
    "plt.yticks(range(d),yticks)\n",
    "plt.ylabel('depth')\n",
    "plt.xlabel('width')\n",
    "plt.xticks(range(w),xticks)\n",
    "plt.colorbar(shrink=.8)\n",
    "plt.title('loss overall of best model')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "plt.imshow(ac_map,cmap='YlGn')\n",
    "plt.yticks(range(d),yticks)\n",
    "plt.xticks(range(w),xticks)\n",
    "plt.ylabel('depth')\n",
    "plt.xlabel('width')\n",
    "plt.colorbar(shrink=.8)\n",
    "plt.title('accuracy of best model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = version = 'base_7l_12u'\n",
    "m = generation[v][best[v]]\n",
    "report_model(m )\n",
    "print(v, m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_model(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_experiment(version=version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "report_experiment(version=version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = curves[version]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for i in h:\n",
    "    l.append(min(i.history['loss']))\n",
    "print(min(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(l) * 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## take a look at the activations inside a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'all_relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create three model clones with decreasing number of layers\n",
    "model3 = generation[version][worst[version]]\n",
    "W = model3.get_weights()\n",
    "\n",
    "model2 = keras.models.clone_model(model3)\n",
    "model2.set_weights(W)\n",
    "model2.pop()\n",
    "\n",
    "model1 = keras.models.clone_model(model3)\n",
    "model1.set_weights(W)\n",
    "model1.pop()\n",
    "model1.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred1 = model1.predict(x=X)\n",
    "print(pred1)\n",
    "plt.imshow(pred1, cmap='RdBu')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pred1.flatten()\n",
    "plt.hist(p)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred2 = model2.predict(x=X)\n",
    "print(pred2)\n",
    "plt.imshow(pred2, cmap='RdBu')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pred2.flatten()\n",
    "plt.hist(p)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred3 = model3.predict(x=X)\n",
    "# print(pred3)\n",
    "print(pred3)\n",
    "plt.imshow(pred3, cmap='RdBu')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pred3.flatten()\n",
    "plt.hist(p)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions by selected model\n",
    "y_hat = model3.predict(x=X).flatten()\n",
    "pos = range(len(Y.flatten()))\n",
    "plt.bar(pos, Y.flatten(), -.8, align='center', color='black', label='true')\n",
    "plt.bar(pos, y_hat, .3, align='edge', color='#3a9362', label='predict')\n",
    "plt.title(\"experiment titled {}\".format(version))\n",
    "plt.xlabel(\"sample image\")\n",
    "plt.ylabel(\"output\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
